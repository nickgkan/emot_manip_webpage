<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  </script>
  <script type="text/javascript"
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1250px;
	}	
	h1 {
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 0px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
  <head>
		<title>Predicting and Manipulating the Emotional Effect of Visual Art</title>
		<meta property="og:image" content="https://richzhang.github.io/ideepcolor/index_files/teaser_v3.jpg"/>
		<meta property="og:title" content="Real-Time User-Guided Image Colorization with Learned Deep Priors. In SIGGRAPH, 2017." />
  </head>

  <body>
    <br>
          <center>
          	<span style="font-size:42px">Predicting and Manipulating the Emotional Effect of Visual Art</span><br>
	  		  <table align=center width=950px>
	  		  <!-- <table align=center width=540px> -->
	  			  <tr>
	  	              <td align=center width=140px>
	  					<center>
	  						<span style="font-size:24px"><a href="https://zfang399.github.io/">Andy Fang</a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=140px>
	  					<center>
	  						<span style="font-size:24px"><a href="https://nickgkan.github.io/">Nikolaos Gkanatsios</a></span>
		  		  		</center>
		  		  	  </td>
	  			  </tr>
			  </table>
          	<!-- <span style="font-size:16px">(*indicates equal contribution)</span><br> -->
          	<span style="font-size:18px"><b>Carnegie Mellon University</b></span>
	  		  <table align=center width=900px>
	  			  <tr>
	  	              <td align=center width=240px>
	  					<center>
	  						<span style="font-size:22px">Code <a href='https://github.com/nickgkan/emotion_manipulation'> [GitHub]</a></span>
		  		  		</center>
		  		  	  </td>
			  </table>
          </center>

  		  <table align=center width=900px>
  			  <tr>
  	              <td width=400px>
  					<center>
  	                	<img class="rounded" src = "./index_files/teaser_v3.jpg" height="275px"></img>
  	                	<br>
					</center>
  	              </td>
  	              </tr>
  	              </table>

  		  <br>
		  <hr>

  		  <table align=center width=900px>
	  		 	<center><h1>Abstract</h1></center>
                   The view of a painting arouses multiple emotions, different for each person. We explore the relation between objective visual cues and subjective affective experiences with the training of an expert model that is able to both classify and alter the emotional effect of visual artworks. First, we train a standard sentiment classifier in a multi-label fashion. Then, we gain insight by i) visualizing gradients to map the emotion back to image regions and ii) statistically analyzing humans' textual descriptions of their feelings about each painting. We discover that although the visual cues often focus on situations or facial expressions, people rely on memories and impression, the latter mostly activated by color patterns. Motivated by this analysis, we train an Energy-Based Model (EBM) to manipulate the emitted emotions of a painting. During training, our model learns both to apply color transformations to minimize the energy of the target class and to contrast images from different emotion classes. In inference, the EBM is given an image and a target emotion and iteratively updates the image to maximize the desired emotional effect. We demonstrate the effectiveness of our approach with a qualitative analysis.
                   <br>
		  <hr>

  		  <table align=center width=900px>
	  		 	<center><h1>Video Presentation</h1></center>
  			  <tr>
<!--   	              <td width=400px>
  					<center>
  	                	<img class="rounded" src = "./index_files/teaser_v3.jpg" height="275px"></img>
  	                	<br>
					</center>
  	              </td> -->
		  		  <table align=center width=600px>
		  		    <tr>
		              <td align=center width=600px>
						<iframe width="800" height="500" src="https://www.youtube.com/embed/XMAOdG7Yw1E" frameborder="0" allowfullscreen></iframe>
					  </td>
					</tr>
				  </table>
				<br>
                </tr>
  		  </table>

  		  <br>
		  <hr>

        <table align=center width=900px>
            <center><h1>Introduction</h1></center>
            What do people feel when admiring an artwork? Are some pixels more attractive given the psychological status of a human? How can we edit an artwork to intensify a desired emotion? We address these questions from a computer vision perspective by learning to classify and manipulate the emotional effects of paintings. Our work steps towards bridging artificial and emotional intelligence and can have immediate application on assistive digital art. For example, designers could consult our emotion-aware model towards more appealing web pages, eye-capturing advertisements and school-book artwork that could motivate kids to study and discuss. Another face of the same coin is emotion-based prediction: what actions could a painting inspire?
            <br><br>
            Reasoning about emotions directly from visual stimuli is a challenging task. First, emotions are subjective. For example, in Fig. 1, there are diverse annotations for the same painting. This can be attributed to the psychological status of humans, as well as their background, experiences and personality. Another challenge is focus: there are no ground-truth salient parts annotated, contrary to object detection for instance. In fact, in the view of a painting, humans may focus on objects, e.g. the "table" in Fig 1, colors, background, e.g. "sky ... getting darker", or even memories, e.g. "remind me of the fall".
            <br><br>
            While emotion classification from images can be tackled as a multi-label problem due to co-existing labels, an important question is where this classifier should focus on. We attempt to answer this question by examining the classifier's gradients [12] directly. We notice that, different from humans, typical classifiers [4] focus mostly on situations, expressions and objects in the foreground; no colors or memories. Thus, manipulating these paintings without any contraints, e.g. training a generative model from scratch, may get stuck in altering parts that are either hard to modify realistically, e.g. facial expressions, or are less salient for humans.
            <br><br>
            In this work we are mainly interested in three aspects: 1. train deep networks to classify emotions from paintings; 2. study where these networks "look" at to identify specific regions of the image that convey specific emotions; 3. alter the emotion of painting by learning to edit particular characteristics. To this end, we first finetune [4] on our set of emotion classes in a multi-label scheme. Then, we employ Grad-CAM [12] to highlight the most salient image regions for any given emotion label. Lastly, we train an Energy-based Model [3] (EBM) to apply minimal color edits on the image and minimize an energy functional conditioned on the emotion. We evaluate our approach on the recently introduced ArtEmis dataset [1]. Our key contributions are a novel, color-conditioned art-emotion manipulation EBM, as well as the first qualitative analysis and interpretation of painting-to-emotion classifiers.
            <br><br>
            <table align=center width=1250px>
                <tr>
                    <td align=center width=1250px>
                        <center>
                            <td><img class="round" style="width:1250px" src="./resources/intro_fig_emot.png"/></td>
                        </center>
                    </td>
                </tr>
            </table>
            <table align=center width=1250px>
                <center>
                    <tr>
                        <td>
                            Figure 1: Emotion classification from paintings is highly subjective. Different people focus on diverse details of the painting and absorb varying emotions. A common thing many people focus on is color.
                        </td>
                    </tr>
                </center>
            </table>
        <hr>

        <table align=center width=900px>
            <center><h1>Related work</h1></center>
            <b>Sentiment analysis</b> has been a popular task in the natural language processing domain [9], yet rare in computer vision, due to its subjective and non-local nature. Prior literature based on visual art considers only sentiment classification [10,6] or emotion-conditioned captioning [1]. We are the first to explore emotion-conditioned editing of images, especially art.
            <br><br>
            <b>Generative models</b>: On a technical aspect, our work lies close to style transferring [5] and implicit conditional generation [7] with EBMs. However, current style transferring approaches attempt to change what is "seen". For example, in style transferring between paintings of two artists, we can always locate characteristics of each painter in the image. This is not true for emotions, where even humans cannot locate precisely the specific areas that arouse a given emotion. At the same time, EBMs are usually applied on synthetic setups [11,7], more rarely on real images [2,8] and never on unconstrained images, such as paintings, that may not even respect physical laws.
		</table>
        <hr>

        <table align=center width=1250px>
            <tr>
                <td align=center width=1250px>
                    <center>
                        <td><img class="round" style="width:1250px" src="./resources/artemis.png"/></td>
                    </center>
                </td>
            </tr>
        </table>
        <table align=center width=1250px>
            <center>
                <tr>
                    <td>
                        Figure 2: <b>Left</b>: Class distribution on ArtEmis dataset [1]. <b>Right</b>: Usage of color language per emotion on the same dataset.
                    </td>
                </tr>
            </center>
        </table>

        <table align=center width=900px>
            <center><h1>Approach</h1></center>
            <b>Classification</b>: We build our classifier on ImageNet-pretrained ResNet [4]. Specifically, we replace the last fully-connected layer with a new one for prediction of our classes, 10 in number. We use ResNet-34 for this task and freeze all blocks except the last one. Batch normalization layer are kept frozen throughout the whole layer. We use Adam optimizer with learning rate 0.001 and a batch size of 128. Our network converges in 15 epochs.
            <br><br>
            <b>Visualization</b> We use Grad-CAM [12] to visualize the image parts that maximize the activation of a target emotion. To do this, we first perform a forward pass through our classifier and obtain a score for the target emotion. We want to maximize this activation, so we can consider that our loss is the negative (logarithm) of this activation. Then, we backpropagate to compute the gradients of this loss wrt the image. The gradients are maximized on locations that are important to attend to when inferring the target emotion.
            <br><br>
            <b>Emotion manipulation</b> Energy-Based Models (EBMs) are a family of networks that output a scalar energy value $E_{\theta}(x)$ which measures the combatibility of the input $x$ and some constraints. Inspired by physics, where stable systems lie on smooth local optima of an energy function, the smaller the EBM output, the better the input satisfies the imposed constraints. During training, the EBM learns the positive data distribution $p_{\theta}(x) \propto e^{-E_{\theta}(x)}$ by minimizing the loss
            \begin{equation}
                \mathcal{L} = \mathbb{E}_{x^{+} \sim p_D}E_{\theta}(x^{+}) - \mathbb{E}_{x^{-} \sim p_{\theta}}E_{\theta}(x^{-})
            \end{equation}
            where $x^{+}$ a positive sample for the true data distribution $p_D$ and $x^{-}$ a negative sample drawn from the learned distribution $p_{\theta}$. To sample from $p_{\theta}$, we start from an initial estimate $x^0$ and refine using Langevin Dynamics [13]:
            \begin{equation}
                x^{k+1} = x^{k} -\lambda \nabla_x E_{\theta}(x^{k})
            \end{equation}
            After $K$ iterations, we obtain $x^{-}=x^{K}$. This formulation is intuitive: we follow the EBM's gradients wrt to the input to update the input in a way that minimizes the energy. This is what we described for visualization earlier. These artificially good samples as treated as negatives. At convergence, the EBM is able to produce very realistic ``negatives'' using Eq. 2.
            <br><br>
            To incorporate the above formulation into our problem, we use ResNet-18 as an EBM. We train a separate energy model for each target emotion. During training, we sample positive examples from the dataset, i.e. images labeled with the target emotion. An open choice is how to sample negatives. Motivated by our visualizations, the fact that people often ground their emotions on colors, as well as the difficulty of image generation from scratch, we learn a colorization task. Specifically, we pick $x^0$ to be the grayscale version of the positive samples, then we use the EBM to colorize it. Intuitively, the EBM should converge to the ground-truth image colors in order to minimize the energy, although there is colorization loss, i.e. we do not compare the learned and ground-truth colors at any point, we only contrast their energies. Lastly, we note that this colorization task itself is not sufficient in order to associate the energy with the emotion. For this reason, we also sample dataset images that are not labeled with the target emotion and treat them as negatives in Eq. 1. We pick $K=20$ and $\lambda=10$ and use the same hyperparameters as in the classification experiment.

        <hr>

 		<center><h1>Results on Legacy Photos</h1></center>

		We trained our system on 1.3M color photos, which were made grayscale "synthetically" (by removing the color components). Here, we show some examples on <i>legacy</i> grayscale photographs. This is Figure 10 in our full paper.

		<br>
  		  <table align=center width=600px>
  		    <tr>
              <td align=center width=600px>
        		<!-- <a href="./index_files/legacy_v4.jpg"><img class="rounded"  src = "./index_files/legacy_v4_small.jpg" width = "800px"></a><br> -->
        		<a href="http://colorization.eecs.berkeley.edu/siggraph/legacy_photos/"><img class="rounded"  src = "./index_files/legacy_v4_small.jpg" width = "800px"></a><br>
				<span style="font-size:16px"></span>
			  </td>
			</tr>
  			  <!-- <tr> -->
  	              <td align=center width=600px>
  					<center>
  						<span style="font-size:28px"><a href='http://colorization.eecs.berkeley.edu/siggraph/legacy_photos/'>Selected Legacy Photos</a></span>
	  		  		</center>
	  		  	  </td>
  		  	  <!-- </tr> -->
		  </table>

		<br>
		<hr>

 		<center><h1>Additional Results</h1></center>

 		We show all of the results from our user study. Each user spent just 1 minute on each image. Each of the 28 users was given minimal training (short 2 minute explanation, and a few questions), and given 10 images to colorize. We show all 280 examples in the link below.

 		This is an extension of Figures 4 & 5 of our paper. Please see Section 4.2 of our paper for additional details.

  		  <table align=center width=600px>
  		    <tr>
              <td align=center width=600px>
        		<a href="http://colorization.eecs.berkeley.edu/siggraph/user_study/"><img class="rounded"  src = "./index_files/imagenet_showcase_small.jpg" width = "800px"></a><br>
        		<!-- <a href="./index_files/imagenet_showcase.jpg"><img class="rounded"  src = "./index_files/imagenet_showcase_small.jpg" width = "800px"></a><br> -->
				<span style="font-size:16px"></span>
			  </td>
			</tr>
  			  <tr>
  	              <td align=center width=600px>
  					<center>
  						<span style="font-size:28px"><a href='http://colorization.eecs.berkeley.edu/siggraph/user_study/'> Full User Study Results</a></span>
	  		  		</center>
	  		  	  </td>
  		  	  </tr>
		  </table>
		<br>

 		We show additional examples of our network incorporating global histogram information. Please see Sections 3.3 and 4.4 for additional details. This is an extension of Figure 9 in our paper.

  		  <table align=center width=600px>
  		    <tr>
              <td align=center width=600px>
        		<!-- <a href="./index_files/lab_all_figures45k.jpg"><img class="rounded" src = "./index_files/lab_all_figures45k_small.jpg" width = "800px"></a><br> -->
        		<a href="http://colorization.eecs.berkeley.edu/siggraph/global_transfer/"><img class="rounded" src = "./index_files/lab_all_figures45k_small.jpg" width = "800px"></a><br>
				<span style="font-size:16px"></span>
			  </td>
			</tr>
  			  <tr>
  	              <td align=center width=600px>
  					<center>
  						<span style="font-size:28px"><a href='http://colorization.eecs.berkeley.edu/siggraph/global_transfer/'>Random Histogram Transfer Results</a></span>
	  		  		</center>
	  		  	  </td>
  		  	  </tr>
		  </table>
		<br>

<!--   		  <table align=center width=600px>
  		  	<tr>
  	              <td align=center width=300px>
  					<center>
  						<span style="font-size:22px"><a href='http://colorization.eecs.berkeley.edu/siggraph/global_transfer/'>Histogram Transfer Results</a></span>
	  		  		</center>
	  		  	  </td>
	  		</tr>
		  </table>
 -->
		  <hr>

 		<center><h1>Try the network</h1></center>
		  <!-- The GitHub page has scripts for fetching our model, and a slightly modified version of Caffe which does color pre-processing (if so desired). We recommend going to the <a href='https://github.com/richzhang/splitbrainauto'>GitHub</a> page and following instructions in the readme. -->

  		  <table align=center width=900px>
  			  <tr>
  	                <td align=center width=900px>
  					<center>
						  <td><a href='https://github.com/junyanz/interactive-deep-colorization'><img class="round" style="width:900px" src="./index_files/network4.jpg"/></a></td>
	  		  		</center>
	  		  		</td>
			  </tr>
		  </table>

  		  <table align=center width=800px>
			  <tr><center>
				<span style="font-size:28px">GitHub <a href='https://github.com/junyanz/interactive-deep-colorization'>[Official Repository]</a>
				<span style="font-size:28px"><a href='https://github.com/richzhang/colorization-pytorch'>[PyTorch Training]</a>
			  <br>
			  </center></tr>
			  <!-- <tr><center> -->
 			<!-- <span style="font-size:28px"><a>&nbsp;Model: [Prototxt] Weights: [Unrescaled] [Rescaled]</a></span> -->
			  <!-- </center></tr> -->
		  </table>

  		  <br>
		  <hr>

  		  <table align=center width=540 px>
	 		<center><h1>Paper</h1></center>
  			  <tr>
				  <td><a href="https://arxiv.org/abs/1705.02999"><img class="layered-paper-big" style="height:175px" src="./index_files/page1.png"/></a></td>
				  <td><span style="font-size:12pt">R. Zhang*, J.Y. Zhu*, P. Isola, <br> X. Geng, A. S. Lin, T. Yu, A. A. Efros.</span><br>
				  <b><span style="font-size:12pt">Real-Time User-Guided Image Colorization with Learned Deep Priors.</b></span><br>
				  <span style="font-size:12pt">In SIGGRAPH, 2017. (hosted on <a href="https://arxiv.org/abs/1705.02999">arXiv</a>)</span></a>
				  <span style="font-size:4pt"><a href=""><br></a>
				  </span>
				  </td>
  	              </td>
              </tr>
  		  </table>
		  <br>

		  <table align=center width=600px>
			  <tr>
				  <td><span style="font-size:24pt"><center>
				  	<a href="./index_files/bibtex_siggraph2017.txt">[Bibtex]</a>
  	              </center></td>
              </tr>
  		  </table>

  		  <br>
		  <hr>
		  	
  		  <table align=center width=1100px>
  			  <tr>
  	              <td width=400px>
  					<left>
	  		  <center><h1>References</h1></center>

                        [1] Panos Achlioptas, Maks Ovsjanikov, Kilichbek Haydarov, Mohamed Elhoseiny, and Leonidas Guibas.
                        <b>Artemis: Affective language for art.</b>
                        CVPR, 2021.[2]
                        <br><br>
                        Yilun Du, Shuang Li, J. Tenenbaum, and Igor Mordatch.
                        <b>Improved contrastive divergence training of energy based models.</b>
                        ArXiv, abs/2012.01316, 2020.
                        <br><br>
                        [3] Will Grathwohl, Kuan-Chieh Wang, Jorn-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi,and Kevin Swersky.
                        <b>Your classifier is secretly an energy based model and you should treat it like one.</b>
                        ICLR, 2020.
                        <br><br>
                        [4] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
                        <b>Deep residual learning for image recognition.</b>
                        CVPR, 2016.
                        <br><br>
                        [5] Tero Karras, S. Laine, and Timo Aila.\
                        <b>A style-based generator architecture for generative adversarial networks.</b>
                        CVPR, 2019.
                        <br><br>
                        [6] H. Kim, Yeong-Seok Kim, S. Kim, and In-Kwon Lee.
                        <b>Building emotional machines: Recognizing image emotions through deep neural networks.</b>
                        IEEE Transactions on Multimedia, 20, 2018.
                        <br><br>
                        [7] Shaomeng Li, Yilun Du, Gido M. van de Ven, A. Torralba, and Igor Mordatch.
                        <b>Energy-based models for continual learning.</b>
                        ArXiv, abs/2011.12216, 2020.
                        <br><br>
                        [8] Yilun Du and Igor Mordatch.
                        <b>Implicit generation and modeling with energy based models.</b>
                        NeurIPS, 2019.
                        <br><br>
                        [9] Walaa Medhat, Ahmed Hassan, and H. Korashy.
                        <b>Sentiment analysis algorithms and applications: A survey.</b>
                        Ain Shams Engineering Journal, 5:1093–1113, 2014.
                        <br><br>
                        [10] Saif M. Mohammad and Svetlana Kiritchenko.
                        <b>Wikiart emotions: An annotated dataset of emotions evoked by art.</b>
                        LREC, 2018.
                        <br><br>
                        [11] Igor Mordatch.
                        <b>Concept learning with energy-based models.</b>
                        ICLR, 2018.
                        <br><br>
                        [12] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra.
                        <b>Grad-cam: Visual explanations from deep networks via gradient-based localization.</b>
                        ICCV, 2017.
                        <br><br>
                        [13] Max Welling and Yee Whye Teh.
                        <b>Bayesian learning via stochastic gradient langevin dynamics.</b>
                        ICML, 2011.

			</left>
		</td>
			 </tr>
		</table>
		<hr>


		<br><br>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75863369-3', 'auto');
  ga('send', 'pageview');

</script>
              
</body>
</html>
 