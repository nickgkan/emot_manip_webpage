<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  </script>
  <script type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1250px;
	}	
	h1 {
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 0px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
  <head>
		<title>Predicting and Manipulating the Emotional Effect of Visual Art</title>
		<meta property="og:image" content="https://richzhang.github.io/ideepcolor/index_files/teaser_v3.jpg"/>
		<meta property="og:title" content="Real-Time User-Guided Image Colorization with Learned Deep Priors. In SIGGRAPH, 2017." />
  </head>

  <body>
    <br>
          <center>
          	<span style="font-size:42px">Predicting and Manipulating the Emotional Effect of Visual Art</span><br>
	  		  <table align=center width=950px>
	  		  <!-- <table align=center width=540px> -->
	  			  <tr>
	  	              <td align=center width=140px>
	  					<center>
	  						<span style="font-size:24px"><a href="https://zfang399.github.io/">Andy Fang</a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=140px>
	  					<center>
	  						<span style="font-size:24px"><a href="https://nickgkan.github.io/">Nikolaos Gkanatsios</a></span>
		  		  		</center>
		  		  	  </td>
	  			  </tr>
			  </table>
          	<!-- <span style="font-size:16px">(*indicates equal contribution)</span><br> -->
          	<span style="font-size:18px"><b>Carnegie Mellon University</b></span>
	  		  <table align=center width=900px>
	  			  <tr>
	  	              <td align=center width=240px>
	  					<center>
	  						<span style="font-size:22px">Code <a href='https://github.com/nickgkan/emotion_manipulation'> [GitHub]</a></span>
		  		  		</center>
		  		  	  </td>
			  </table>
          </center>

  		  <center>
          <br>
		  <div>
		  	  <figure style="margin: 0;">
		  	  	<figcaption>Original Image &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp 
		  	  	Edited Image (add contentment) &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp 
		  	    Editing Process &nbsp &nbsp</figcaption>
			    <img src="./resources/vis/good_before_3.png" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >
			    <img src="./resources/vis/good_after_3.png" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >
			    <img src="./resources/vis/good_move_3.gif" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >
			  </figure>
		  </div>
		  </center>
  		  <br>
		  <hr>

  		  <table align=center width=900px>
	  		 	<center><h1>Abstract</h1></center>
                   The view of a painting arouses multiple emotions, different for each person. We explore the relation between objective visual cues and subjective affective experiences with the training of an expert model that is able to both classify and alter the emotional effect of visual artworks. First, we train a standard sentiment classifier in a multi-label fashion. Then, we gain insight by i) visualizing gradients to map the emotion back to image regions and ii) statistically analyzing humans' textual descriptions of their feelings about each painting. We discover that although the visual cues often focus on situations or facial expressions, people rely on memories and impression, the latter mostly activated by color patterns. Motivated by this analysis, we train an Energy-Based Model (EBM) to manipulate the emitted emotions of a painting. During training, our model learns both to apply color transformations to minimize the energy of the target class and to contrast images from different emotion classes. In inference, the EBM is given an image and a target emotion and iteratively updates the image to maximize the desired emotional effect. We demonstrate the effectiveness of our approach with a qualitative analysis.
                   <br>
		  <hr>

  		  <table align=center width=900px>
	  		 	<center><h1>Video Presentation</h1></center>
  			  <tr>
<!--   	              <td width=400px>
  					<center>
  	                	<img class="rounded" src = "./index_files/teaser_v3.jpg" height="275px"></img>
  	                	<br>
					</center>
  	              </td> -->
		  		  <table align=center width=600px>
		  		    <tr>
		              <td align=center width=600px>
						<iframe width="800" height="500" src="https://www.youtube.com/embed/XMAOdG7Yw1E" frameborder="0" allowfullscreen></iframe>
					  </td>
					</tr>
				  </table>
				<br>
                </tr>
  		  </table>

  		  <br>
		  <hr>

        <table align=center width=900px>
            <center><h1>Introduction</h1></center>
            What do people feel when admiring an artwork? Are some pixels more attractive given the psychological status of a human? How can we edit an artwork to intensify a desired emotion? We address these questions from a computer vision perspective by learning to classify and manipulate the emotional effects of paintings. Our work steps towards bridging artificial and emotional intelligence and can have immediate application on assistive digital art. For example, designers could consult our emotion-aware model towards more appealing web pages, eye-capturing advertisements and school-book artwork that could motivate kids to study and discuss. Another face of the same coin is emotion-based prediction: what actions could a painting inspire?
            <br><br>
            Reasoning about emotions directly from visual stimuli is a challenging task. First, emotions are subjective. For example, in Fig. 1, there are diverse annotations for the same painting. This can be attributed to the psychological status of humans, as well as their background, experiences and personality. Another challenge is focus: there are no ground-truth salient parts annotated, contrary to object detection for instance. In fact, in the view of a painting, humans may focus on objects, e.g. the "table" in Fig 1, colors, background, e.g. "sky ... getting darker", or even memories, e.g. "remind me of the fall".
            <br><br>
            While emotion classification from images can be tackled as a multi-label problem due to co-existing labels, an important question is where this classifier should focus on. We attempt to answer this question by examining the classifier's gradients [12] directly. We notice that, different from humans, typical classifiers [4] focus mostly on situations, expressions and objects in the foreground; no colors or memories. Thus, manipulating these paintings without any contraints, e.g. training a generative model from scratch, may get stuck in altering parts that are either hard to modify realistically, e.g. facial expressions, or are less salient for humans.
            <br><br>
            In this work we are mainly interested in three aspects: 1. train deep networks to classify emotions from paintings; 2. study where these networks "look" at to identify specific regions of the image that convey specific emotions; 3. alter the emotion of painting by learning to edit particular characteristics. To this end, we first finetune [4] on our set of emotion classes in a multi-label scheme. Then, we employ Grad-CAM [12] to highlight the most salient image regions for any given emotion label. Lastly, we train an Energy-based Model [3] (EBM) to apply minimal color edits on the image and minimize an energy functional conditioned on the emotion. We evaluate our approach on the recently introduced ArtEmis dataset [1]. Our key contributions are a novel, color-conditioned art-emotion manipulation EBM, as well as the first qualitative analysis and interpretation of painting-to-emotion classifiers.
            <br><br>
            <table align=center width=1250px>
                <tr>
                    <td align=center width=1250px>
                        <center>
                            <td><img class="round" style="width:1250px" src="./resources/intro_fig_emot.png"/></td>
                        </center>
                    </td>
                </tr>
            </table>
            <table align=center width=1250px>
                <center>
                    <tr>
                        <td>
                            Figure 1: Emotion classification from paintings is highly subjective. Different people focus on diverse details of the painting and absorb varying emotions. A common thing many people focus on is color.
                        </td>
                    </tr>
                </center>
            </table>
        <hr>

        <table align=center width=900px>
            <center><h1>Related work</h1></center>
            <b>Sentiment analysis</b> has been a popular task in the natural language processing domain [9], yet rare in computer vision, due to its subjective and non-local nature. Prior literature based on visual art considers only sentiment classification [10,6] or emotion-conditioned captioning [1]. We are the first to explore emotion-conditioned editing of images, especially art.
            <br><br>
            <b>Generative models</b>: On a technical aspect, our work lies close to style transferring [5] and implicit conditional generation [7] with EBMs. However, current style transferring approaches attempt to change what is "seen". For example, in style transferring between paintings of two artists, we can always locate characteristics of each painter in the image. This is not true for emotions, where even humans cannot locate precisely the specific areas that arouse a given emotion. At the same time, EBMs are usually applied on synthetic setups [11,7], more rarely on real images [2,8] and never on unconstrained images, such as paintings, that may not even respect physical laws.
            <br><br>
            <b>ArtEmis Dataset</b>: Introduced in [1], ArtEmis offers annotations for emotions, artists, styles and human users' comments about each image. There are 10 emotion types in our classification setup, the distribution of which is plotted in Fig. 2 We also use comments to mine common patterns from text. Specifically, as we show in Fig. 2, for all classes, humans often use color language to describe their emotions, e.g. "the colors are very awe-inspiring".
		
        </table>

        <table align=center width=1250px>
            <tr>
                <td align=center width=1250px>
                    <center>
                        <td><img class="round" style="width:1250px" src="./resources/artemis.png"/></td>
                    </center>
                </td>
            </tr>
        </table>
        <table align=center width=1250px>
            <center>
                <tr>
                    <td>
                        Figure 2: <b>Left</b>: Class distribution on ArtEmis dataset [1]. <b>Right</b>: Usage of color language per emotion on the same dataset. The fact that people often acknowledge color pattern as emotion-triggering motivates us to learn to apply color-based transformations to the images
                    </td>
                </tr>
            </center>
        </table>
        <hr>

        <table align=center width=900px>
            <center><h1>Approach</h1></center>
            <b>Classification</b>: We build our classifier on ImageNet-pretrained ResNet [4]. Specifically, we replace the last fully-connected layer with a new one for our 10 emotion classes. We use ResNet-34 for this task and freeze all blocks except the last one. Batch normalization layers are kept frozen throughout the whole network. We use Adam optimizer with learning rate 0.001 and a batch size of 128. Our network converges in 15 epochs.
            <br><br>
            <b>Visualization</b> We use Grad-CAM [12] to visualize the image parts that maximize the activation of a target emotion. To do this, we first perform a forward pass through our classifier and obtain a score for the target emotion. We want to maximize this activation, so we can consider that our loss is the negative (logarithm) of this activation. Then, we backpropagate to compute the gradients of this loss wrt the image. The gradients are maximized on locations that are important to attend to when inferring the target emotion.
            <br><br>
            <b>Emotion manipulation</b> Energy-Based Models (EBMs) are a family of networks that output a scalar energy value $E_{\theta}(x)$ which measures the combatibility of the input $x$ and some constraints. Inspired by physics, where stable systems lie on smooth local optima of an energy function, the smaller the EBM output, the better the input satisfies the imposed constraints. During training, the EBM learns the positive data distribution $p_{\theta}(x) \propto e^{-E_{\theta}(x)}$ by minimizing the loss
            \begin{equation}
                \mathcal{L} = \mathbb{E}_{x^{+} \sim p_D}E_{\theta}(x^{+}) - \mathbb{E}_{x^{-} \sim p_{\theta}}E_{\theta}(x^{-})
            \end{equation}
            where $x^{+}$ a positive sample for the true data distribution $p_D$ and $x^{-}$ a negative sample drawn from the learned distribution $p_{\theta}$. To sample from $p_{\theta}$, we start from an initial estimate $x^0$ and refine using Langevin Dynamics [13]:
            \begin{equation}
                x^{k+1} = x^{k} -\lambda \nabla_x E_{\theta}(x^{k})
            \end{equation}
            After $K$ iterations, we obtain $x^{-}=x^{K}$. This formulation is intuitive: we follow the EBM's gradients wrt to the input to update the input in a way that minimizes the energy. This is what we described for visualization earlier. These artificially good samples as treated as negatives. At convergence, the EBM is able to produce very realistic "negatives" using Eq. 2.
            <br><br>
            To incorporate the above formulation into our problem, we use ResNet-18 as an EBM. We train a separate energy model for each target emotion. During training, we sample positive examples from the dataset, i.e. images labeled with the target emotion. An open choice is how to sample negatives. Motivated by our visualizations and the fact that people often ground their emotions on colors, as well as the difficulty of image generation from scratch, we learn a colorization task. Specifically, we pick $x^0$ to be the grayscale version of the positive samples, then we use the EBM to colorize it. Intuitively, the EBM should converge to the ground-truth image colors in order to minimize the energy, although there is colorization loss, i.e. we do not compare the learned and ground-truth colors at any point, we only contrast their energies. Lastly, we note that this colorization task itself is not sufficient in order to associate the energy with the emotion. For this reason, we also sample dataset images that are not labeled with the target emotion and treat them as negatives in Eq. 1. We pick $K=50$ and $\lambda=20$ and use the same hyperparameters as in the classification experiment.

        <hr>

        <center><h1>Results on Emotion Classification</h1></center>
        <b>Classification</b> We evaluate our trained classifier on ArtEmis and get a mAP 55.01% on 10 classes. However, this accuracy is not equally distributed among classes, as can be seen in the next plots.
        <br>
        <table align=center width=1250px>
            <tr>
                <td align=center width=1250px>
                    <center>
                        <td><img class="round" style="width:1250px" src="./resources/cls_results.png"/></td>
                    </center>
                </td>
            </tr>
        </table>
        <table align=center width=1250px>
            <center>
                <tr>
                    <td>
                        <b>Left</b>: Per-class average precision. We can see vast differences among classes. <b>Mid</b>: Distribution of correct answers: how the total number of correct detections is distributed among classes.. <b>Right</b>: True distribution of classes in ArtEmis. We can observe that the distribution of correct answers matches the label distribution, which suggests that i) there is bias and ii) our model is data-hungry.
                    </td>
                </tr>
            </center>
        </table>
        <br><br>

        <b>Visualizations</b> Below we show the Grad-CAM [12] visualizations of the classification model on unseen images in the test set. On each row we show two examples, where each example shows the original image (left) and the grad-CAM visualization (right) conditioned on the emotion label (caption). We show reasonable results in the first four rows and show results that's not interpretable in the last row. This suggests that the model learns to ground most of its decisions on the most salient image region correlated with the emotion label, but it also makes mistakes where it focuses on regions less correlated with the emotion label.
        Note that all images are padded here so that they have same height and width (hence some black areas).

        <table align=center width=1250px>
            <tr>
                <td align=center width=1250px>
                    <center>
                        <td><img class="round" style="width:1250px" src="./resources/gradcam.png"/></td>
                    </center>
                </td>
            </tr>
        </table>

        <br>

        Below we also show Grad-CAM visualizations of model's prediction the same image but conditioned on different emotion labels. We show the original image (left), and two Grad-CAM visualizations of two different labels (middle and right). We see that the model relies on different part of the image to inform its prediction of different emotions.

        <table align=center width=1250px>
            <tr>
                <td align=center width=1250px>
                    <center>
                        <td><img class="round" style="width:1250px" src="./resources/gradcam2.png"/></td>
                    </center>
                </td>
            </tr>
        </table>

        <br>
        <hr>

 		<center><h1>Results on Emotion Manipulation</h1></center>

		We apply our EBM to unseen images in the test set to qualitatively evaluate the model's ability to enhance the target emotion. The target emotion is <i>contentment</i>, and the input images are all images without the target emotion label. Below are some successful samples of emotion manipulation. We show the original image (left), edited image (middle), and the editing process (right). The model learns to edit colors of saliant regions of the image and enhance the contentment emotion.
        For this task, images are reshaped to $64\times64$.
        
        <center>
          <br>
		  <div>
		  	  <figure style="margin: 0;">
		  	  	<figcaption>Original Image &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp 
		  	  	Edited Image &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp 
		  	    Editing Process &nbsp &nbsp</figcaption>
			    <img src="./resources/vis/good_before_0.png" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >
			    <img src="./resources/vis/good_after_0.png" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >
			    <img src="./resources/vis/good_move_0.gif" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >
			  </figure>
		  </div>

		  <div>
			  <img src="./resources/vis/good_before_1.png" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >

			  <img src="./resources/vis/good_after_1.png" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >
			  
			  <img src="./resources/vis/good_move_1.gif" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >
			  <br>
		  </div>

		  <div>
			  <img src="./resources/vis/good_before_2.png" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >

			  <img src="./resources/vis/good_after_2.png" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >
			  
			  <img src="./resources/vis/good_move_2.gif" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >
			  <br>
		  </div>

		  <div>
			  <img src="./resources/vis/good_before_3.png" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >

			  <img src="./resources/vis/good_after_3.png" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >
			  
			  <img src="./resources/vis/good_move_3.gif" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >
			  <br>
		  </div>

		  <div>
			  <img src="./resources/vis/good_before_4.png" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >

			  <img src="./resources/vis/good_after_4.png" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >
			  
			  <img src="./resources/vis/good_move_4.gif" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >
			  <br>
			  <br>
		  </div>
		</center>

		We also show some failure samples below. We show the original image (left), edited image (middle), and the editing process (right). Compared to the successful ones, we see that our EBM fails when the input image is too abstract to start with, or contains a great amount of emotions opposite to the target emotion. We conjecture it is hard to enhance the target emotion in those cases.

        <center>
          <br>
		  <div>
		  	  <figure style="margin: 0;">
		  	  	<figcaption>Original Image &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp 
		  	  	Edited Image &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp 
		  	    Editing Process &nbsp &nbsp</figcaption>
			    <img src="./resources/vis/bad_before_0.png" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >
			    <img src="./resources/vis/bad_after_0.png" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >
			    <img src="./resources/vis/bad_move_0.gif" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >
			  </figure>
		  </div>

		  <div>
			  <img src="./resources/vis/bad_before_1.png" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >

			  <img src="./resources/vis/bad_after_1.png" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >
			  
			  <img src="./resources/vis/bad_move_1.gif" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >
			  <br>
		  </div>

		  <div>
			  <img src="./resources/vis/bad_before_2.png" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >

			  <img src="./resources/vis/bad_after_2.png" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >
			  
			  <img src="./resources/vis/bad_move_2.gif" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >
			  <br>
		  </div>

		  <div>
			  <img src="./resources/vis/bad_before_3.png" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >

			  <img src="./resources/vis/bad_after_3.png" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >
			  
			  <img src="./resources/vis/bad_move_3.gif" alt="{{hist}}" width="256" height="256" style="margin: 0px 30px 0px 0px;" >
			  <br>
		  </div>
		</center>

		<br>
		<hr>
        <table align=center width=900px>
            <center><h1>Conclusions and Future Steps</h1></center>
            In this project we classify, interpret and manipulate the emotions evoked by paintings.
            Our manipulation network is a lightweight energy-based model that does not need a discriminator (contrary to GANs) or exhaustive tuning to work.
            We display a multitude of qualitative results to examine the capability of our model to iteratively update paintings in a way that i) <b>at every step the outcome is a valid painting</b>, ii) the <b>shape is preserved</b>, only colors change and iii) <b>the effect of target emotion is maximized</b>.
            <br><br>
            However, this is still an open problem. On a technical aspect, we are able to manipulate relatively small images ($64\times64$) and we show results for a specific and frequent emotion. Training on larger images and more emotions is a natural next step. At the same time, more rigorous evaluation methods are needed.
            On the research side, we focus on modifying colors only. While this restriction is motivated by the statistics of the language humans use, color is not only stimuli for emotion. Future works should explore the idea of uncostrained manipulation. Our preliminary experiments during this project indicate that this is an extremely challenging task to solve.
            Lastly, if humans often refer to colors and we are able to modify images using this information, could we edit images based on other similar cues? Language-guided image editing can be an exciting future direction.
        </table>
  		  <br>
		  <hr>
		  	
  		  <table align=center width=1100px>
  			  <tr>
  	              <td width=400px>
  					<left>
	  		  <center><h1>References</h1></center>

                        [1] Panos Achlioptas, Maks Ovsjanikov, Kilichbek Haydarov, Mohamed Elhoseiny, and Leonidas Guibas.
                        <b>Artemis: Affective language for art.</b>
                        CVPR, 2021
                        <br><br>
                        [2] Yilun Du, Shuang Li, J. Tenenbaum, and Igor Mordatch.
                        <b>Improved contrastive divergence training of energy based models.</b>
                        ArXiv, abs/2012.01316, 2020.
                        <br><br>
                        [3] Will Grathwohl, Kuan-Chieh Wang, Jorn-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi,and Kevin Swersky.
                        <b>Your classifier is secretly an energy based model and you should treat it like one.</b>
                        ICLR, 2020.
                        <br><br>
                        [4] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
                        <b>Deep residual learning for image recognition.</b>
                        CVPR, 2016.
                        <br><br>
                        [5] Tero Karras, S. Laine, and Timo Aila.\
                        <b>A style-based generator architecture for generative adversarial networks.</b>
                        CVPR, 2019.
                        <br><br>
                        [6] H. Kim, Yeong-Seok Kim, S. Kim, and In-Kwon Lee.
                        <b>Building emotional machines: Recognizing image emotions through deep neural networks.</b>
                        IEEE Transactions on Multimedia, 20, 2018.
                        <br><br>
                        [7] Shaomeng Li, Yilun Du, Gido M. van de Ven, A. Torralba, and Igor Mordatch.
                        <b>Energy-based models for continual learning.</b>
                        ArXiv, abs/2011.12216, 2020.
                        <br><br>
                        [8] Yilun Du and Igor Mordatch.
                        <b>Implicit generation and modeling with energy based models.</b>
                        NeurIPS, 2019.
                        <br><br>
                        [9] Walaa Medhat, Ahmed Hassan, and H. Korashy.
                        <b>Sentiment analysis algorithms and applications: A survey.</b>
                        Ain Shams Engineering Journal, 5:1093–1113, 2014.
                        <br><br>
                        [10] Saif M. Mohammad and Svetlana Kiritchenko.
                        <b>Wikiart emotions: An annotated dataset of emotions evoked by art.</b>
                        LREC, 2018.
                        <br><br>
                        [11] Igor Mordatch.
                        <b>Concept learning with energy-based models.</b>
                        ICLR, 2018.
                        <br><br>
                        [12] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra.
                        <b>Grad-cam: Visual explanations from deep networks via gradient-based localization.</b>
                        ICCV, 2017.
                        <br><br>
                        [13] Max Welling and Yee Whye Teh.
                        <b>Bayesian learning via stochastic gradient langevin dynamics.</b>
                        ICML, 2011.

			</left>
		</td>
			 </tr>
		</table>
		<hr>


		<br><br>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75863369-3', 'auto');
  ga('send', 'pageview');

</script>
              
</body>
</html>
 